- プログラムの動作について
main.c を gcc でコンパイルし、learned.bin が配置されたディレクトリ内で実行すればプログラムが動作する。

- プログラムの設計について
プログラムは main.c と learning.c の大きく 2 つに分かれる。
main.c は課題において要求されているプログラムであり、learning.c は下記の遺伝的アルゴリズムで学習を行うためのプログラムである。
それ以外の各ヘッダファイルの詳細は以下の通りである。
・piece.h : 駒の状態や動きを表す型、関数が置かれている
・game_state.h : 盤面全体の状態を管理するための型や関数が置かれている
・move.h : 駒の動きを表す型と、駒の動きのルールに関わる判定関数が置かれている
・gogo_controller.h : 5五将棋の全体的な処理を行う関数が置かれている
・ai.h : AI に関連する型や関数が置かれている
・random.h : 質の良い乱数生成器 (PCG) が置かれている
・ga.h : 遺伝的アルゴリズムの実装が置かれている

- AI について
・評価関数について
評価関数については、二つの観点から評価値を決めるように設計した。
一つ目は、駒の価値である。駒の価値については、以下のものを参考にした。
http://minerva.cs.uec.ac.jp/~ito/entcog/contents/lecture/date/5-kakinoki.pdf
駒の価値について、自分たちで遺伝的アルゴリズムなどを用いて、良い値を探していくのもいいかとも考えたが、
手元で上の評価値を基に対戦してみた結果、参考にした上記の駒の価値は、かなり適切な評価値を振ってあるように見受けられたので、ほとんどそのままの値を使用した。
二つ目は、相手玉と自分の駒の相対的な位置についてである。これは ai_seed 構造体に三次元配列 table[8][9][5] として記録されている。
最初の次元は、全 8 種類 (成り銀、と金、金は動きが同じなので同種類とカウントしている) の駒の種類を表している。次の次元は相手玉と自分の駒の縦方向(列方向)の相対位置を表しており、-4, -3, ... , +3, +4 の 9 個になっている。
最後の次元は、相手玉と自分の駒の横方向(行方向)の相対位置を表しており、これは縦方向とほぼ同じだが、5五将棋は駒の動きが左右対象なので 0, ... , +4 だけを考えれば良く、配列のサイズを 5 に抑えている。
この table[8][9][5] については、下記の遺伝的アルゴリズムを用いることで良い評価値を学習している。

・遺伝的アルゴリズムについて
提出したプログラムでは、上記の table の評価値を定めるために遺伝的アルゴリズム (以下 GA) を使用している。table はバイト数が 360 と大きいことから個体数は 200 とし、代わりに学習における世代数を減らすことで実行時間を削減している。
各世代においては、以下の方法で次の世代となる遺伝子を決めている。
1. 200 体の中から 20 体をランダムに選ぶ
2. 20 体で総当りのトーナメントを行い、上位 7 体を選出する
3. 1, 2, 3 位はそのまま採用する
4. 上位 7 体から 2 つ選ぶ組み合わせのうち、(4位, 7位), (5位, 6位), (5位, 7位), (6位, 7位) の四組を除いた組について遺伝子の交叉を行い採用する
5. 採用した合計 20 の遺伝子のうち、1 位だったもの以外を確率的に突然変異させる
6. 以上の操作を 10 回繰り返す
交叉においては各遺伝子から半分づつ遺伝子の成分を組み合わせ、その値に確率的なブレ (-5～+5) を加算する。
突然変異においては遺伝子の一部を元の値とは一切関係ないランダムな値に変更する。突然変異が起こる確率は 1/30 である。
またここで用いる乱数には (そしてプログラムの他の場所で用いる乱数にも)、PCG と呼ばれる線形合同法の改良版のアルゴリズムを用いている。その実装には https://github.com/rkern/pcg64 を大きく参考にした。

・探索について
αβ法に深さの制限をつけて行いました。また反復深化の手法も取り入れました。
それぞれの関数について：
    alpha_beta_max():
        AIのターンの時、その盤面の評価値を返す。
        評価値の決め方は以下。
            ・深さが限界に達したとき、または勝敗が決しているときは評価関数の値をそのまま返す。
            ・そうでないとき、次の行先を列挙し、順番に行先の評価値を調べる(alpha_beta_minの値を得る)。
            評価値がαより大きければ、αをその評価値に更新する。      
            αがβより大きくなったら、それ以降の探索は無意味なので、いまのβ値を返す。
            途中で切り上げることなく最後の候補まで探索が終わったら、αを返す。
    alpha_beta_min():
        プレイヤーのターンの時、その盤面の評価値を返す。
        評価値の決め方は以下
            ・深さが限界に達したとき、または勝敗が決しているときは評価関数の値をそのまま返す。
            ・そうでないとき、次の行先を列挙し、順番に行先の評価値を調べる(alpha_beta_maxの値を得る)。
            評価値がβより小さければ、βをその評価値に更新する。      
            βがαより小さくなったら、それ以降の探索は無意味なので、いまのαの値を返す。
            途中で切り上げることなく最後の候補まで探索が終わったら、βを返す。
    ai_decide_move():
        基本はalpha_beta_max()と一緒。
        ただし、αを更新した際に最善手を決めるようになっている点、また、反復深化を採用している点で異なる。
        反復深化では、探索の深さを１ずつ増やしていく。
        まえの探索の時点での最善手から探索を始めることで、高速化を期待している。
    