AIについて
・評価関数について
評価関数については、二つの観点から評価値を決めるように設計した。
一つ目は、駒の価値である。駒の価値については、以下のものを参考にした。
http://minerva.cs.uec.ac.jp/~ito/entcog/contents/lecture/date/5-kakinoki.pdf
駒の価値について、自分たちで遺伝的アルゴリズムなどを用いて、良い値を探していくのもいいかとも考えたが、
手元で上の評価値を基に対戦してみた結果、参考にした上記の駒の価値は、かなり適切な評価値を振ってあるように見受けられたので、ほとんどそのままの値を使用した。
二つ目は、相手玉と自分の駒の相対的な位置についてである。これは ai_seed 構造体に三次元配列 table[8][9][5] として記録されている。
最初の次元は、全 8 種類 (成り銀、と金、金は動きが同じなので同種類とカウントしている) の駒の種類を表している。次の次元は相手玉と自分の駒の縦方向(列方向)の相対位置を表しており、-4, -3, ... , +3, +4 の 9 個になっている。
最後の次元は、相手玉と自分の駒の横方向(行方向)の相対位置を表しており、これは縦方向とほぼ同じだが、5五将棋は駒の動きが左右対象なので 0, ... , +4 だけを考えれば良く、配列のサイズを 5 に抑えている。
この table[8][9][5] については、下記の遺伝的アルゴリズムを用いることで良い評価値を学習している。

・遺伝的アルゴリズムについて

・探索について
αβ法に深さの制限をつけて行いました。また反復深化の手法も取り入れました。
それぞれの関数について：
    alpha_beta_max():
        AIのターンの時、その盤面の評価値を返す。
        評価値の決め方は以下。
            ・深さが限界に達したとき、または勝敗が決しているときは評価関数の値をそのまま返す。
            ・そうでないとき、次の行先を列挙し、順番に行先の評価値を調べる。
            評価値がαより大きければ、αをその評価値に更新する。      
            αがβより大きくなったら、それ以降の探索は無意味なので、いまのβ値を返す。
            途中で切り上げることなく最後の候補まで探索が終わったら、αを返す。
    alpha_beta_min():
        プレイヤーのターンの時、その盤面の評価値を返す。
        評価値の決め方は以下
            ・深さが限界に達したとき、または勝敗が決しているときは評価関数の値をそのまま返す。
            ・そうでないとき、次の行先を列挙し、順番に行先の評価値を調べる。
            評価値がβより小さければ、βをその評価値に更新する。      
            βがαより小さくなったら、それ以降の探索は無意味なので、いまのαの値を返す。
            途中で切り上げることなく最後の候補まで探索が終わったら、βを返す。
    ai_decide_move():
        基本はalpha_beta_max()と一緒。
        ただし、αを更新した際に最善手を決めるようになっている点、また、反復深化を採用している点で異なる。
        反復深化では、探索の深さを１ずつ増やしていく。
        まえの探索の時点での最善手から探索を始めることで、高速化を期待している。
    