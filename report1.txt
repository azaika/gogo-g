探索について
αβ法に深さの制限をつけて行いました。また反復深化の手法も取り入れました。
それぞれの関数について：
    alpha_beta_max():
        AIのターンの時、その盤面の評価値を返す。
        評価値の決め方は以下。
            ・深さが限界に達したとき、または勝敗が決しているときは評価関数の値をそのまま返す。
            ・そうでないとき、次の行先を列挙し、順番に行先の評価値を調べる。
            評価値がαより大きければ、αをその評価値に更新する。      
            αがβより大きくなったら、それ以降の探索は無意味なので、いまのβ値を返す。
            途中で切り上げることなく最後の候補まで探索が終わったら、αを返す。
    alpha_beta_min():
        プレイヤーのターンの時、その盤面の評価値を返す。
        評価値の決め方は以下
            ・深さが限界に達したとき、または勝敗が決しているときは評価関数の値をそのまま返す。
            ・そうでないとき、次の行先を列挙し、順番に行先の評価値を調べる。
            評価値がβより小さければ、βをその評価値に更新する。      
            βがαより小さくなったら、それ以降の探索は無意味なので、いまのαの値を返す。
            途中で切り上げることなく最後の候補まで探索が終わったら、βを返す。
    ai_decide_move():
        基本はalpha_beta_max()と一緒。
        ただし、αを更新した際に最善手を決めるようになっている点、また、反復深化を採用している点で異なる。
        反復深化では、探索の深さを１ずつ増やしていく。
        まえの探索の時点での最善手から探索を始めることで、高速化を期待している。
    